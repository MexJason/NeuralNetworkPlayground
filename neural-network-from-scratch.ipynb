{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPUvEj6DFb2SS44ZhYSDHi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MexJason/NeuralNetworkPlayground/blob/main/neural-network-from-scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "tNQFZrVsPJnX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BuZmK6ZPDva",
        "outputId": "288bea52-eb72-4ff6-980d-8ea651e884ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[-7.60455520e-03, -1.65839048e-02, -9.36033798e-03,\n",
              "          3.31282554e-04,  8.32794740e-03,  2.12257625e-02,\n",
              "          6.40499753e-03,  1.40248664e-02, -1.39409386e-02,\n",
              "         -1.01519045e-02, -2.43724231e-03,  7.24446823e-03],\n",
              "        [-1.19410944e-03, -9.10544091e-03, -1.38655811e-02,\n",
              "          3.12102856e-03,  1.78979356e-02,  7.21372790e-04,\n",
              "          1.05815797e-02, -8.87024815e-03, -2.83426700e-03,\n",
              "          1.09582872e-02,  1.53741555e-02,  5.64490349e-03],\n",
              "        [ 1.66184138e-02, -7.65801911e-03,  1.07363349e-02,\n",
              "         -4.20968125e-03,  8.31167623e-04, -9.13677570e-03,\n",
              "          5.05999374e-03,  1.92484641e-03, -9.63352655e-03,\n",
              "         -9.23306217e-03, -6.88818268e-03, -2.96735458e-03],\n",
              "        [-7.76746790e-04,  3.08099156e-02,  9.82272589e-03,\n",
              "          7.79891441e-03,  3.16508248e-03,  5.98805620e-03,\n",
              "         -6.21404727e-03,  2.82495936e-03,  6.91802912e-03,\n",
              "         -4.37977102e-04, -1.50613825e-02, -5.72443556e-03],\n",
              "        [-6.62063805e-03, -7.64760058e-03, -1.33984431e-02,\n",
              "         -5.26627133e-03,  6.92386742e-03, -1.33192905e-03,\n",
              "          1.17813070e-02,  6.26220640e-03,  9.20768516e-03,\n",
              "          1.00068472e-02,  2.32282686e-03,  4.42817283e-03],\n",
              "        [-3.28470288e-03,  5.84968660e-03,  9.51123948e-03,\n",
              "         -1.00328636e-02,  3.80152603e-03, -4.30464700e-03,\n",
              "          6.28923828e-03,  1.10730724e-02,  2.46791050e-03,\n",
              "          1.79756705e-02, -3.80645550e-03, -3.68965603e-03],\n",
              "        [ 2.18810528e-03,  4.43903026e-03, -1.55265525e-02,\n",
              "          2.11610003e-05,  1.06364761e-02,  1.97047423e-03,\n",
              "          1.60140287e-02, -1.52130981e-02,  2.85358465e-03,\n",
              "         -4.53907612e-03, -2.45616326e-04,  6.33527256e-03],\n",
              "        [ 1.51319623e-02,  1.13398670e-03,  5.71001237e-03,\n",
              "          3.39930132e-03, -4.32043079e-03, -3.55384493e-03,\n",
              "         -8.69813896e-04, -2.17155634e-03,  8.16448939e-03,\n",
              "         -4.28175434e-03, -4.90318865e-03, -1.86500319e-02]]),\n",
              " array([[-0.01709119, -0.00727562, -0.00066933, -0.01409534, -0.0058439 ,\n",
              "          0.00271764, -0.00749961,  0.00284869],\n",
              "        [-0.00205868, -0.00180923,  0.00624137,  0.00356983, -0.00779468,\n",
              "          0.00722389,  0.01656193,  0.00898394],\n",
              "        [-0.01833002,  0.00366348,  0.01633449,  0.00630077,  0.0022035 ,\n",
              "          0.00469313, -0.00831429,  0.00757602],\n",
              "        [ 0.00982938, -0.00629156,  0.01256068, -0.02378441,  0.00728684,\n",
              "          0.00454316,  0.00579301, -0.00400352]]),\n",
              " array([[-0.00677223,  0.00153548, -0.00061999, -0.00331659]]))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "num_layers = 3\n",
        "layer_size = [12, 8, 4, 1]\n",
        "input_size = 12\n",
        "\n",
        "W1 = np.random.randn(layer_size[1], input_size)* 0.01\n",
        "b1 = np.zeros((layer_size[1],1))\n",
        "W2 = np.random.randn(layer_size[2], layer_size[1])* 0.01\n",
        "b2 = np.zeros((layer_size[2],1))\n",
        "W3 = np.random.randn(layer_size[3], layer_size[2])* 0.01\n",
        "b3 = np.zeros((layer_size[3],1))\n",
        "\n",
        "params = [W1, b1, W2, b2, W3, b3]\n",
        "W1, W2, W3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of paramaters in model \n",
        "num_params = sum(p.size for p in params)\n",
        "num_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vsl_LYGUPIp6",
        "outputId": "7a4b7233-52ae-4fe1-a361-33eacdd6b5a6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "145"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer_dims = [12, 8, 4, 1]\n",
        "\n",
        "parameters = {}\n",
        "L = len(layer_dims) # number of layers in the network\n",
        "\n",
        "for l in range(1, L):\n",
        "    parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
        "    parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "    \n",
        "#num_params = sum(p.size for p in parameters)\n",
        "num_params = 0\n",
        "for p in parameters:\n",
        "  print(p, parameters[p].shape)\n",
        "  num_params += parameters[p].size\n",
        "print(num_params)"
      ],
      "metadata": {
        "id": "FdX-0VBqQRTu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b98dea8-b70b-422e-c37d-5d98a770ec52"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W1 (8, 12)\n",
            "b1 (8, 1)\n",
            "W2 (4, 8)\n",
            "b2 (4, 1)\n",
            "W3 (1, 4)\n",
            "b3 (1, 1)\n",
            "145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))"
      ],
      "metadata": {
        "id": "ibnX_esJzNx8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_forward(A, W, b):\n",
        "    Z = W @ A + b\n",
        "    cache = (A, W, b)\n",
        "      \n",
        "    return Z, cache"
      ],
      "metadata": {
        "id": "OfOzE0z1r7mC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "  if activation == \"sigmoid\":\n",
        "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "    A = sigmoid(Z)\n",
        "    activation_cache = A * (1 - A)\n",
        "  elif activation == \"relu\":\n",
        "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "    A = np.maximum(0, Z)\n",
        "    activation_cache = A * (1 - A)\n",
        "\n",
        "  cache = (linear_cache, activation_cache)\n",
        "\n",
        "  return A, cache"
      ],
      "metadata": {
        "id": "k3u9pxTYtfIh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def L_model_forward(X, parameters):\n",
        "  \"\"\"\n",
        "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
        "    \n",
        "    Arguments:\n",
        "    X -- data, numpy array of shape (input size, number of examples)\n",
        "    parameters -- output of initialize_parameters_deep()\n",
        "    \n",
        "    Returns:\n",
        "    AL -- activation value from the output (last) layer\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() (there are L of them, indexed from 0 to L-1)\n",
        "    \"\"\"\n",
        "  caches = []\n",
        "  A = X\n",
        "  L = len(parameters) // 2           \n",
        "\n",
        "  for l in range(1, L):\n",
        "      A_prev = A \n",
        "      A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)],'relu')\n",
        "      caches.append(cache)\n",
        "  # MAKE SURE TO USE CURRENT A!!!\n",
        "  AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)],'sigmoid')\n",
        "  caches.append(cache)\n",
        "\n",
        "\n",
        "  print(AL.shape, len(caches))\n",
        "  return AL, caches"
      ],
      "metadata": {
        "id": "6iOQtYt5tmww"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculations we will need in backward pass \n",
        "def linear_backward(dZ, cache):\n",
        "  A_prev, W, b = cache\n",
        "  m = A_prev.shape[1]\n",
        "  \n",
        "  dW = dZ @ A_prev.T * (1/m)\n",
        "  db = np.sum(dZ, axis=1, keepdims=True)* (1/m)\n",
        "  dA_prev = W.T @ dZ\n",
        "\n",
        "  return dA_prev, dW, db"
      ],
      "metadata": {
        "id": "ff9GXX_Xvvmu"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu_backward(dA, activation_cache):\n",
        "  Z = activation_cache\n",
        "  dZ = np.array(dA, copy=True)\n",
        "  dZ[Z <= 0] = 0\n",
        "    \n",
        "  return dZ\n",
        "\n",
        "def sigmoid_backward(dA, activation_cache):\n",
        "  Z = activation_cache\n",
        "  s = 1/(1+np.exp(-Z))\n",
        "  dZ = dA * s * (1-s)\n",
        "    \n",
        "  return dZ"
      ],
      "metadata": {
        "id": "U63DjofzkxJE"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_activation_backward(dA, cache, activation):\n",
        "    linear_cache, activation_cache = cache\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "      dZ = relu_backward(dA, activation_cache)\n",
        "      dA_prev, dW, db = linear_backward(dZ, linear_cache)       \n",
        "\n",
        "    elif activation == \"sigmoid\":\n",
        "      dZ = sigmoid_backward(dA, activation_cache)\n",
        "      dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "\n",
        "    return dA_prev, dW, db"
      ],
      "metadata": {
        "id": "ODGlm35_XSbr"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def L_model_backward(AL, Y, caches):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
        "    \n",
        "    Arguments:\n",
        "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
        "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
        "    \n",
        "    Returns:\n",
        "    grads -- A dictionary with the gradients\n",
        "             grads[\"dA\" + str(l)] = ... \n",
        "             grads[\"dW\" + str(l)] = ...\n",
        "             grads[\"db\" + str(l)] = ... \n",
        "    \"\"\"\n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
        "    \n",
        "    # Initializing the backpropagation\n",
        "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        "    \n",
        "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
        "    current_cache = caches[L-1]\n",
        "    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
        "    grads[\"dA\" + str(L-1)] = dA_prev_temp\n",
        "    grads[\"dW\" + str(L)] = dW_temp\n",
        "    grads[\"db\" + str(L)] = db_temp\n",
        "    \n",
        "    # Loop from l=L-2 to l=0\n",
        "    for l in reversed(range(L-1)):\n",
        "        # lth layer: (RELU -> LINEAR) gradients.\n",
        "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
        "        \n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)], current_cache, \"relu\")\n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    return grads"
      ],
      "metadata": {
        "id": "cYWeWJvNkHtU"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(params, grads, learning_rate):\n",
        "    parameters = params.copy()\n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    # Update rule for each parameter. Use a for loop.\n",
        "    #(â‰ˆ 2 lines of code)\n",
        "    for l in range(L):\n",
        "        # parameters[\"W\" + str(l+1)] = ...\n",
        "        # parameters[\"b\" + str(l+1)] = ...\n",
        "        # YOUR CODE STARTS HERE\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "nCoICwv6kX1y"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_dims = [12, 8, 4]\n",
        "X = np.random.randn(layer_dims[0], layer_dims[1])\n",
        "Y = np.random.randn(layer_dims[-1], layer_dims[-2])\n",
        "print(Y.shape)\n",
        "learning_rate = 0.1\n",
        "\n",
        "parameters = {}\n",
        "L = len(layer_dims) # number of layers in the network\n",
        "\n",
        "for l in range(1, L):\n",
        "    parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
        "    parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "\n",
        "for i in range(50):\n",
        "\n",
        "  # Forward Pass\n",
        "  AL, caches = L_model_forward(X, parameters)\n",
        "\n",
        "  # Backward Pass\n",
        "  grads = L_model_backward(AL, Y, caches)\n",
        "\n",
        "  # Update\n",
        "  params = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "print(\"W1 = \"+ str(params[\"W1\"]))\n",
        "print(\"b1 = \"+ str(params[\"b1\"]))\n",
        "print(\"W2 = \"+ str(params[\"W2\"]))\n",
        "print(\"b2 = \"+ str(params[\"b2\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mg0WA87rnPlR",
        "outputId": "cc9938c4-349a-42b4-e831-c1f394e02993"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 8)\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "(12, 8)\n",
            "1 (8, 8)\n",
            "(4, 8) 2\n",
            "W1 = [[-0.00875519  0.00451392 -0.00047041  0.00647549 -0.00423294 -0.01421044\n",
            "   0.02074365 -0.01398679 -0.01486948 -0.01787945 -0.0052235  -0.03331159]\n",
            " [ 0.00441255 -0.00192163 -0.0157988   0.00023813  0.02114324 -0.01447794\n",
            "   0.00777244 -0.00303146 -0.01298744 -0.00777681 -0.00767231  0.02319273]\n",
            " [-0.01605497 -0.00161229 -0.01081788  0.0123154   0.00327132 -0.00909443\n",
            "  -0.01087341 -0.00417058 -0.00072541 -0.00128715  0.00549088 -0.00830598]\n",
            " [ 0.00683894  0.00404388  0.00099203 -0.01482533  0.00596388 -0.02558409\n",
            "   0.00474579  0.01367511 -0.00653249  0.0123945  -0.02984664  0.00611309]\n",
            " [ 0.00398266 -0.00065628  0.01490505 -0.00224224  0.00480961  0.01606126\n",
            "   0.01553561 -0.01647395  0.00703649 -0.00613685  0.00153747 -0.00399327]\n",
            " [-0.00855197  0.00496879 -0.00047967 -0.00344911  0.01489328  0.00574919\n",
            "   0.00615806 -0.00531632  0.0126659   0.00299801 -0.00502325  0.00098075]\n",
            " [ 0.01180892 -0.01418342  0.01078364  0.00915926  0.00681115 -0.01063895\n",
            "  -0.01081841  0.01398623  0.00312667 -0.01162121 -0.01085989  0.01800989]\n",
            " [ 0.01190528 -0.0070896  -0.01219676  0.01237698  0.0072364  -0.00112913\n",
            "   0.00560129 -0.00088444 -0.01737157  0.00211112  0.00881551  0.00433061]]\n",
            "b1 = [[ 6.56030288e-04]\n",
            " [-5.17195942e-05]\n",
            " [ 3.19833191e-04]\n",
            " [ 1.60092597e-04]\n",
            " [ 2.40554505e-05]\n",
            " [-1.23223608e-04]\n",
            " [-4.64809180e-05]\n",
            " [ 1.71323099e-04]]\n",
            "W2 = [[ 0.00069994 -0.02192282 -0.01052609  0.00093505  0.00034266  0.00466233\n",
            "   0.00328912 -0.00688257]\n",
            " [ 0.01121253 -0.00095695 -0.00418849  0.00042691  0.00791682 -0.01368984\n",
            "   0.00694653 -0.01722267]\n",
            " [-0.02014252 -0.00536646 -0.0066414  -0.00961198  0.00824975  0.00695912\n",
            "   0.0256675  -0.00022839]\n",
            " [ 0.00047255  0.00370098 -0.01483478 -0.00398911  0.0108829  -0.00679244\n",
            "  -0.0053097  -0.01000177]]\n",
            "b2 = [[-0.03751696]\n",
            " [ 0.03122196]\n",
            " [-0.02874744]\n",
            " [-0.0248594 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bt8nih1wkZ4x"
      },
      "execution_count": 48,
      "outputs": []
    }
  ]
}