{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMI8PV3ch0srOYGaeUXYcXa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MexJason/NeuralNetworkPlayground/blob/main/neural-network-from-scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "tNQFZrVsPJnX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BuZmK6ZPDva",
        "outputId": "288bea52-eb72-4ff6-980d-8ea651e884ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[-7.60455520e-03, -1.65839048e-02, -9.36033798e-03,\n",
              "          3.31282554e-04,  8.32794740e-03,  2.12257625e-02,\n",
              "          6.40499753e-03,  1.40248664e-02, -1.39409386e-02,\n",
              "         -1.01519045e-02, -2.43724231e-03,  7.24446823e-03],\n",
              "        [-1.19410944e-03, -9.10544091e-03, -1.38655811e-02,\n",
              "          3.12102856e-03,  1.78979356e-02,  7.21372790e-04,\n",
              "          1.05815797e-02, -8.87024815e-03, -2.83426700e-03,\n",
              "          1.09582872e-02,  1.53741555e-02,  5.64490349e-03],\n",
              "        [ 1.66184138e-02, -7.65801911e-03,  1.07363349e-02,\n",
              "         -4.20968125e-03,  8.31167623e-04, -9.13677570e-03,\n",
              "          5.05999374e-03,  1.92484641e-03, -9.63352655e-03,\n",
              "         -9.23306217e-03, -6.88818268e-03, -2.96735458e-03],\n",
              "        [-7.76746790e-04,  3.08099156e-02,  9.82272589e-03,\n",
              "          7.79891441e-03,  3.16508248e-03,  5.98805620e-03,\n",
              "         -6.21404727e-03,  2.82495936e-03,  6.91802912e-03,\n",
              "         -4.37977102e-04, -1.50613825e-02, -5.72443556e-03],\n",
              "        [-6.62063805e-03, -7.64760058e-03, -1.33984431e-02,\n",
              "         -5.26627133e-03,  6.92386742e-03, -1.33192905e-03,\n",
              "          1.17813070e-02,  6.26220640e-03,  9.20768516e-03,\n",
              "          1.00068472e-02,  2.32282686e-03,  4.42817283e-03],\n",
              "        [-3.28470288e-03,  5.84968660e-03,  9.51123948e-03,\n",
              "         -1.00328636e-02,  3.80152603e-03, -4.30464700e-03,\n",
              "          6.28923828e-03,  1.10730724e-02,  2.46791050e-03,\n",
              "          1.79756705e-02, -3.80645550e-03, -3.68965603e-03],\n",
              "        [ 2.18810528e-03,  4.43903026e-03, -1.55265525e-02,\n",
              "          2.11610003e-05,  1.06364761e-02,  1.97047423e-03,\n",
              "          1.60140287e-02, -1.52130981e-02,  2.85358465e-03,\n",
              "         -4.53907612e-03, -2.45616326e-04,  6.33527256e-03],\n",
              "        [ 1.51319623e-02,  1.13398670e-03,  5.71001237e-03,\n",
              "          3.39930132e-03, -4.32043079e-03, -3.55384493e-03,\n",
              "         -8.69813896e-04, -2.17155634e-03,  8.16448939e-03,\n",
              "         -4.28175434e-03, -4.90318865e-03, -1.86500319e-02]]),\n",
              " array([[-0.01709119, -0.00727562, -0.00066933, -0.01409534, -0.0058439 ,\n",
              "          0.00271764, -0.00749961,  0.00284869],\n",
              "        [-0.00205868, -0.00180923,  0.00624137,  0.00356983, -0.00779468,\n",
              "          0.00722389,  0.01656193,  0.00898394],\n",
              "        [-0.01833002,  0.00366348,  0.01633449,  0.00630077,  0.0022035 ,\n",
              "          0.00469313, -0.00831429,  0.00757602],\n",
              "        [ 0.00982938, -0.00629156,  0.01256068, -0.02378441,  0.00728684,\n",
              "          0.00454316,  0.00579301, -0.00400352]]),\n",
              " array([[-0.00677223,  0.00153548, -0.00061999, -0.00331659]]))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "num_layers = 3\n",
        "layer_size = [12, 8, 4, 1]\n",
        "input_size = 12\n",
        "\n",
        "W1 = np.random.randn(layer_size[1], input_size)* 0.01\n",
        "b1 = np.zeros((layer_size[1],1))\n",
        "W2 = np.random.randn(layer_size[2], layer_size[1])* 0.01\n",
        "b2 = np.zeros((layer_size[2],1))\n",
        "W3 = np.random.randn(layer_size[3], layer_size[2])* 0.01\n",
        "b3 = np.zeros((layer_size[3],1))\n",
        "\n",
        "params = [W1, b1, W2, b2, W3, b3]\n",
        "W1, W2, W3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of paramaters in model \n",
        "num_params = sum(p.size for p in params)\n",
        "num_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vsl_LYGUPIp6",
        "outputId": "7a4b7233-52ae-4fe1-a361-33eacdd6b5a6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "145"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer_dims = [12, 8, 4, 1]\n",
        "\n",
        "parameters = {}\n",
        "L = len(layer_dims) # number of layers in the network\n",
        "\n",
        "for l in range(1, L):\n",
        "    parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
        "    parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "    \n",
        "#num_params = sum(p.size for p in parameters)\n",
        "num_params = 0\n",
        "for p in parameters:\n",
        "  print(p, parameters[p].shape)\n",
        "  num_params += parameters[p].size\n",
        "print(num_params)"
      ],
      "metadata": {
        "id": "FdX-0VBqQRTu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b98dea8-b70b-422e-c37d-5d98a770ec52"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W1 (8, 12)\n",
            "b1 (8, 1)\n",
            "W2 (4, 8)\n",
            "b2 (4, 1)\n",
            "W3 (1, 4)\n",
            "b3 (1, 1)\n",
            "145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))"
      ],
      "metadata": {
        "id": "ibnX_esJzNx8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_forward(A, W, b):\n",
        "    Z = W @ A + b\n",
        "    cache = (A, W, b)\n",
        "      \n",
        "    return Z, cache"
      ],
      "metadata": {
        "id": "OfOzE0z1r7mC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "  if activation == \"sigmoid\":\n",
        "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "    A = sigmoid(Z)\n",
        "    activation_cache = A * (1 - A)\n",
        "  elif activation == \"relu\":\n",
        "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "    A = np.maximum(0, Z)\n",
        "    activation_cache = A * (1 - A)\n",
        "\n",
        "  cache = (linear_cache, activation_cache)\n",
        "\n",
        "  return A, cache"
      ],
      "metadata": {
        "id": "k3u9pxTYtfIh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def L_model_forward(X, parameters):\n",
        "  \"\"\"\n",
        "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
        "    \n",
        "    Arguments:\n",
        "    X -- data, numpy array of shape (input size, number of examples)\n",
        "    parameters -- output of initialize_parameters_deep()\n",
        "    \n",
        "    Returns:\n",
        "    AL -- activation value from the output (last) layer\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() (there are L of them, indexed from 0 to L-1)\n",
        "    \"\"\"\n",
        "  caches = []\n",
        "  A = X\n",
        "  L = len(parameters) // 2           \n",
        "\n",
        "  for l in range(1, L):\n",
        "      A_prev = A \n",
        "      A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)],'relu')\n",
        "      caches.append(cache)\n",
        "  # MAKE SURE TO USE CURRENT A!!!\n",
        "  AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)],'sigmoid')\n",
        "  caches.append(cache)\n",
        "\n",
        "\n",
        "  return AL, caches"
      ],
      "metadata": {
        "id": "6iOQtYt5tmww"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculations we will need in backward pass \n",
        "def linear_backward(dZ, cache):\n",
        "  A_prev, W, b = cache\n",
        "  m = A_prev.shape[1]\n",
        "  \n",
        "  dW = dZ @ A_prev.T * (1/m)\n",
        "  db = np.sum(dZ, axis=1, keepdims=True)* (1/m)\n",
        "  dA_prev = W.T @ dZ\n",
        "\n",
        "  return dA_prev, dW, db"
      ],
      "metadata": {
        "id": "ff9GXX_Xvvmu"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu_backward(dA, activation_cache):\n",
        "  Z = activation_cache\n",
        "  dZ = np.array(dA, copy=True)\n",
        "  dZ[Z <= 0] = 0\n",
        "    \n",
        "  return dZ\n",
        "\n",
        "def sigmoid_backward(dA, activation_cache):\n",
        "  Z = activation_cache\n",
        "  s = 1/(1+np.exp(-Z))\n",
        "  dZ = dA * s * (1-s)\n",
        "    \n",
        "  return dZ"
      ],
      "metadata": {
        "id": "U63DjofzkxJE"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_activation_backward(dA, cache, activation):\n",
        "    linear_cache, activation_cache = cache\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "      dZ = relu_backward(dA, activation_cache)\n",
        "      dA_prev, dW, db = linear_backward(dZ, linear_cache)       \n",
        "\n",
        "    elif activation == \"sigmoid\":\n",
        "      dZ = sigmoid_backward(dA, activation_cache)\n",
        "      dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "\n",
        "    return dA_prev, dW, db"
      ],
      "metadata": {
        "id": "ODGlm35_XSbr"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def L_model_backward(AL, Y, caches):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
        "    \n",
        "    Arguments:\n",
        "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
        "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
        "    \n",
        "    Returns:\n",
        "    grads -- A dictionary with the gradients\n",
        "             grads[\"dA\" + str(l)] = ... \n",
        "             grads[\"dW\" + str(l)] = ...\n",
        "             grads[\"db\" + str(l)] = ... \n",
        "    \"\"\"\n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
        "    \n",
        "    # Initializing the backpropagation\n",
        "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        "    \n",
        "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
        "    current_cache = caches[L-1]\n",
        "    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
        "    grads[\"dA\" + str(L-1)] = dA_prev_temp\n",
        "    grads[\"dW\" + str(L)] = dW_temp\n",
        "    grads[\"db\" + str(L)] = db_temp\n",
        "    \n",
        "    # Loop from l=L-2 to l=0\n",
        "    for l in reversed(range(L-1)):\n",
        "        # lth layer: (RELU -> LINEAR) gradients.\n",
        "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
        "        \n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)], current_cache, \"relu\")\n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    return grads"
      ],
      "metadata": {
        "id": "cYWeWJvNkHtU"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(params, grads, learning_rate):\n",
        "    parameters = params.copy()\n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    # Update rule for each parameter. Use a for loop.\n",
        "    #(≈ 2 lines of code)\n",
        "    for l in range(L):\n",
        "        # parameters[\"W\" + str(l+1)] = ...\n",
        "        # parameters[\"b\" + str(l+1)] = ...\n",
        "        # YOUR CODE STARTS HERE\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "nCoICwv6kX1y"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_dims = [12, 8, 4]\n",
        "X = np.random.randn(layer_dims[0], layer_dims[1])\n",
        "Y = np.random.randn(layer_dims[-1], layer_dims[-2])\n",
        "print(Y.shape)\n",
        "learning_rate = 0.1\n",
        "\n",
        "parameters = {}\n",
        "L = len(layer_dims) # number of layers in the network\n",
        "\n",
        "for l in range(1, L):\n",
        "    parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
        "    parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "\n",
        "for i in range(50):\n",
        "\n",
        "  # Forward Pass\n",
        "  AL, caches = L_model_forward(X, parameters)\n",
        "\n",
        "  # Backward Pass\n",
        "  grads = L_model_backward(AL, Y, caches)\n",
        "\n",
        "  # Update\n",
        "  params = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "print(\"W1 = \"+ str(params[\"W1\"]))\n",
        "print(\"b1 = \"+ str(params[\"b1\"]))\n",
        "print(\"W2 = \"+ str(params[\"W2\"]))\n",
        "print(\"b2 = \"+ str(params[\"b2\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mg0WA87rnPlR",
        "outputId": "ef161897-cb57-4675-9aca-538c690a3e28"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 8)\n",
            "W1 = [[-7.73157537e-03  5.56945533e-03  1.66607133e-02  7.44950362e-03\n",
            "   1.01146772e-02 -8.51487043e-03  1.85758996e-03  1.70757763e-02\n",
            "   1.53078050e-02 -8.16580974e-03 -4.26450034e-03 -1.19278061e-02]\n",
            " [ 1.12465760e-02 -7.34046814e-03 -1.37295409e-02  5.68597899e-03\n",
            "  -1.59094428e-02  1.11439367e-02  6.16833286e-03 -5.80261578e-03\n",
            "  -2.54008158e-03 -2.44379992e-02 -8.58995803e-03 -1.68561483e-02]\n",
            " [-5.68660562e-03 -4.61843032e-03 -1.41438767e-02  6.65935087e-03\n",
            "  -2.70432770e-03  1.28512333e-02  1.08178487e-02  6.57094585e-03\n",
            "  -1.46582756e-02  1.92741557e-03 -2.58386606e-03  3.62571569e-03]\n",
            " [-3.89394303e-03 -5.91551288e-03  6.98233960e-04  1.16800326e-02\n",
            "   3.23267123e-03  1.46094158e-02  9.92888461e-03  1.45533893e-02\n",
            "  -8.18728366e-03 -3.99515078e-03 -1.61905849e-02  1.98405374e-03]\n",
            " [ 4.55679014e-03 -9.59127034e-03 -1.56790896e-02 -1.52821130e-02\n",
            "   1.63132525e-02 -1.11947109e-02  1.01158524e-02  2.28792008e-06\n",
            "   1.47259511e-02 -1.25987446e-02 -1.13116824e-02 -9.56325334e-03]\n",
            " [-3.66492974e-03  1.94615914e-02 -1.41853445e-02 -3.29652006e-02\n",
            "   4.16633136e-03  1.02392690e-02 -1.10162459e-02 -1.33887342e-02\n",
            "   1.00492631e-02 -3.36956396e-03  6.40163117e-03 -1.45945797e-02]\n",
            " [ 1.10954694e-02 -1.16256709e-02 -1.77651129e-02  7.81527809e-04\n",
            "   8.84922375e-03  6.65218841e-03  1.10385616e-02  1.25383301e-04\n",
            "   2.48846042e-03 -1.13563854e-03  2.12340221e-03  1.97529713e-03]\n",
            " [-7.26368224e-03 -7.19888993e-03 -3.37829033e-03  1.83930759e-03\n",
            "  -3.90185086e-03  2.42572809e-02 -1.03057815e-02 -6.22105007e-03\n",
            "   8.17721830e-03  1.01764733e-03  4.02446653e-03 -8.89408139e-03]]\n",
            "b1 = [[-4.30716461e-05]\n",
            " [ 1.23341871e-03]\n",
            " [ 1.82709531e-04]\n",
            " [ 3.39224430e-04]\n",
            " [ 1.75992703e-03]\n",
            " [ 8.86244063e-04]\n",
            " [-5.53032622e-04]\n",
            " [ 2.96997505e-04]]\n",
            "W2 = [[-0.00102632 -0.01336713 -0.00209456  0.00765712 -0.00146031 -0.0053366\n",
            "   0.01391444  0.00661889]\n",
            " [ 0.00346175 -0.00948876  0.00061553 -0.00349936 -0.00352494  0.01224545\n",
            "   0.00708333 -0.01148797]\n",
            " [ 0.00112882 -0.01767142 -0.01398407 -0.00734961 -0.02055155 -0.01269359\n",
            "  -0.00245268 -0.00419113]\n",
            " [-0.00451144 -0.00843812 -0.00096036  0.00486119 -0.02031733 -0.00374294\n",
            "   0.01105429  0.00148037]]\n",
            "b2 = [[-0.02021508]\n",
            " [-0.03634452]\n",
            " [-0.0523695 ]\n",
            " [-0.02876108]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bt8nih1wkZ4x"
      },
      "execution_count": 73,
      "outputs": []
    }
  ]
}